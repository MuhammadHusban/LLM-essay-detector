{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6971638,"sourceType":"datasetVersion","datasetId":3961875}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-22T13:30:32.641933Z","iopub.execute_input":"2024-05-22T13:30:32.642377Z","iopub.status.idle":"2024-05-22T13:30:33.622738Z","shell.execute_reply.started":"2024-05-22T13:30:32.642344Z","shell.execute_reply":"2024-05-22T13:30:33.621572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\nfile_path = '/kaggle/input/llm-7-prompt-training-dataset/train_essays_7_prompts.csv'\ndata = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:30:33.625066Z","iopub.execute_input":"2024-05-22T13:30:33.626186Z","iopub.status.idle":"2024-05-22T13:30:34.546780Z","shell.execute_reply.started":"2024-05-22T13:30:33.626145Z","shell.execute_reply":"2024-05-22T13:30:34.545567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n\n# Basic Data Analysis\n# Distribution of labels\nlabel_counts = data['label'].value_counts()\n\n# Length of essays\ndata['essay_length'] = data['text'].apply(lambda x: len(word_tokenize(x)))\n\n# Text Data Analysis\n# Most common words\nnltk.download('punkt')\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\nall_words = ' '.join(data['text']).lower()\nall_words_tokens = word_tokenize(all_words)\nfiltered_words = [word for word in all_words_tokens if word.isalnum() and word not in stop_words]\nword_freq = Counter(filtered_words)\n\n# Visualization\n# Distribution of essay lengths\nplt.figure(figsize=(10, 6))\nsns.histplot(data['essay_length'], kde=True)\nplt.title('Distribution of Essay Lengths')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\nplt.show()\n\n# Bar chart for the frequency of common words\ncommon_words_df = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Frequency'])\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Frequency', y='Word', data=common_words_df)\nplt.title('Top 20 Most Common Words')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.show()\n\n# Word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(filtered_words))\n\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Essay Texts')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:30:45.615189Z","iopub.execute_input":"2024-05-22T13:30:45.615559Z","iopub.status.idle":"2024-05-22T13:34:15.249759Z","shell.execute_reply.started":"2024-05-22T13:30:45.615531Z","shell.execute_reply":"2024-05-22T13:34:15.248409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom collections import Counter\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Function to clean and tokenize text\ndef clean_and_tokenize(text):\n    text = re.sub(r'\\W+', ' ', text.lower())\n    tokens = text.split()\n    return tokens\n\n# Basic Data Analysis\n# Distribution of labels\nlabel_counts = data['label'].value_counts()\n\n# Length of essays\ndata['essay_length'] = data['text'].apply(lambda x: len(clean_and_tokenize(x)))\n\n# Text Data Analysis\n# Most common words\nfiltered_words = [word for text in data['text'] for word in clean_and_tokenize(text)]\nword_freq = Counter(filtered_words)\n\n# Visualization\n# Label distribution\nplt.figure(figsize=(8, 8))\nplt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140)\nplt.title('Distribution of AI-generated vs Human-generated Essays')\nplt.axis('equal')\nplt.show()\n\n# Average essay length by label\navg_essay_length = data.groupby('label')['essay_length'].mean().reset_index()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='label', y='essay_length', data=avg_essay_length)\nplt.title('Average Essay Length by Label')\nplt.xlabel('Label')\nplt.ylabel('Average Number of Words')\nplt.xticks([0, 1], ['AI-generated', 'Human-generated'])\nplt.show()\n\n# Common words by label\nai_text = ' '.join(data[data['label'] == 0]['text'])\nhuman_text = ' '.join(data[data['label'] == 1]['text'])\n\nai_words = clean_and_tokenize(ai_text)\nhuman_words = clean_and_tokenize(human_text)\n\nai_word_freq = Counter(ai_words)\nhuman_word_freq = Counter(human_words)\n\ncommon_ai_words_df = pd.DataFrame(ai_word_freq.most_common(20), columns=['Word', 'Frequency'])\ncommon_human_words_df = pd.DataFrame(human_word_freq.most_common(20), columns=['Word', 'Frequency'])\n\n# Bar chart for the frequency of common words (AI-generated)\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Frequency', y='Word', data=common_ai_words_df)\nplt.title('Top 20 Most Common Words in AI-generated Essays')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.show()\n\n# Bar chart for the frequency of common words (Human-generated)\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Frequency', y='Word', data=common_human_words_df)\nplt.title('Top 20 Most Common Words in Human-generated Essays')\nplt.xlabel('Frequency')\nplt.ylabel('Word')\nplt.show()\n\n# Bigrams\nvectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\nbigrams_matrix = vectorizer.fit_transform(data['text'])\nbigrams = vectorizer.get_feature_names_out()\nbigram_freq = bigrams_matrix.sum(axis=0).A1\nbigram_freq_df = pd.DataFrame(list(zip(bigrams, bigram_freq)), columns=['Bigram', 'Frequency'])\nbigram_freq_df = bigram_freq_df.sort_values(by='Frequency', ascending=False).head(20)\n\n# Bar chart for the frequency of bigrams\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Frequency', y='Bigram', data=bigram_freq_df)\nplt.title('Top 20 Most Common Bigrams in Essays')\nplt.xlabel('Frequency')\nplt.ylabel('Bigram')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:37:30.285956Z","iopub.execute_input":"2024-05-22T13:37:30.286433Z","iopub.status.idle":"2024-05-22T13:38:05.945824Z","shell.execute_reply.started":"2024-05-22T13:37:30.286402Z","shell.execute_reply":"2024-05-22T13:38:05.944615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n\n# Load the dataset\nfile_path = '/kaggle/input/llm-7-prompt-training-dataset/train_essays_7_prompts.csv'\ndata = pd.read_csv(file_path)\n\n# Preprocess the Data\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\nX = tfidf_vectorizer.fit_transform(data['text'])\ny = data['label']\n\n# Split the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the Model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Evaluate the Model\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\n# Display the results\nprint(\"Model Performance:\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\nClassification Report:\")\nprint(class_report)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:39:21.752675Z","iopub.execute_input":"2024-05-22T13:39:21.753163Z","iopub.status.idle":"2024-05-22T13:39:28.848639Z","shell.execute_reply.started":"2024-05-22T13:39:21.753126Z","shell.execute_reply":"2024-05-22T13:39:28.847508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Get feature names\nfeature_names = tfidf_vectorizer.get_feature_names_out()\n\n# Get the coefficients from the logistic regression model\ncoefficients = model.coef_[0]\n\n# Create a dataframe for feature importances\nfeature_importances = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': coefficients\n})\n\n# Sort the dataframe by absolute value of the coefficient\nfeature_importances['AbsCoefficient'] = np.abs(feature_importances['Coefficient'])\nfeature_importances = feature_importances.sort_values(by='AbsCoefficient', ascending=False).head(20)\n\n# Plot the feature importances\nplt.figure(figsize=(10, 8))\nsns.barplot(x='Coefficient', y='Feature', data=feature_importances)\nplt.title('Top 20 Most Important Features')\nplt.xlabel('Coefficient Value')\nplt.ylabel('Feature')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:40:08.655127Z","iopub.execute_input":"2024-05-22T13:40:08.655540Z","iopub.status.idle":"2024-05-22T13:40:09.117796Z","shell.execute_reply.started":"2024-05-22T13:40:08.655508Z","shell.execute_reply":"2024-05-22T13:40:09.116600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI-generated', 'Human-generated'], yticklabels=['AI-generated', 'Human-generated'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:40:52.923435Z","iopub.execute_input":"2024-05-22T13:40:52.923887Z","iopub.status.idle":"2024-05-22T13:40:53.240117Z","shell.execute_reply.started":"2024-05-22T13:40:52.923855Z","shell.execute_reply":"2024-05-22T13:40:53.238927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ROC Curve\ny_proba = model.predict_proba(X_test)[:, 1]\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:41:02.030607Z","iopub.execute_input":"2024-05-22T13:41:02.031043Z","iopub.status.idle":"2024-05-22T13:41:02.797467Z","shell.execute_reply.started":"2024-05-22T13:41:02.030980Z","shell.execute_reply":"2024-05-22T13:41:02.796236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Precision-Recall Curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='blue', lw=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:41:11.931600Z","iopub.execute_input":"2024-05-22T13:41:11.932033Z","iopub.status.idle":"2024-05-22T13:41:12.208147Z","shell.execute_reply.started":"2024-05-22T13:41:11.931979Z","shell.execute_reply":"2024-05-22T13:41:12.206918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['AI-generated', 'Human-generated'], yticklabels=['AI-generated', 'Human-generated'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# ROC Curve\ny_proba = model.predict_proba(X_test)[:, 1]\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()\n\n# Precision-Recall Curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='blue', lw=2)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:41:20.078924Z","iopub.execute_input":"2024-05-22T13:41:20.079348Z","iopub.status.idle":"2024-05-22T13:41:20.949494Z","shell.execute_reply.started":"2024-05-22T13:41:20.079317Z","shell.execute_reply":"2024-05-22T13:41:20.948371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\nfile_path = '/kaggle/input/llm-7-prompt-training-dataset/train_essays_7_prompts_v2.csv'\ndata = pd.read_csv(file_path)\n\n# Function to get sample essays\ndef get_sample_essays(data, label, num_samples=3):\n    samples = data[data['label'] == label].sample(n=num_samples, random_state=42)\n    return samples['text'].tolist()\n\n# Get sample AI-generated essays\nai_generated_samples = get_sample_essays(data, label=0, num_samples=3)\nprint(\"Sample AI-generated Essays:\")\nfor i, essay in enumerate(ai_generated_samples, 1):\n    print(f\"\\nEssay {i}:\\n{essay}\\n\")\n\n# Get sample human-generated essays\nhuman_generated_samples = get_sample_essays(data, label=1, num_samples=3)\nprint(\"Sample Human-generated Essays:\")\nfor i, essay in enumerate(human_generated_samples, 1):\n    print(f\"\\nEssay {i}:\\n{essay}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:48:34.977558Z","iopub.execute_input":"2024-05-22T13:48:34.978018Z","iopub.status.idle":"2024-05-22T13:48:35.829145Z","shell.execute_reply.started":"2024-05-22T13:48:34.977972Z","shell.execute_reply":"2024-05-22T13:48:35.828053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to preprocess and predict\ndef predict_essay(text):\n    # Preprocess the text\n    text_transformed = tfidf_vectorizer.transform([text])\n    \n    # Make a prediction\n    prediction = model.predict(text_transformed)\n    \n    # Map prediction to label\n    label_map = {0: 'AI-generated', 1: 'Human-generated'}\n    return label_map[prediction[0]]\n\n# Example usage\nexample_essay_ai = \"\"\"INTRODUCTION\n\nThe Mona Lisa is one paint that is famous in the world, according to some new computer software can reconise emotions in people, this system is a innovation of Prof. Thomas Haung, of the Beckman and is working with Prof. Nicu Sebe, they will have help from Dr. Paul Eckman institute of Advanced Science at the University of Illinois man, creator crator of FACS.\n\nBODY\n\nThis system has some questions that people thikn, howcan a computer recognize subtle facial movements we humans use to express how we feel? Dr. Haung said that the procces begins when the computer structs a 3-D computer model of the face; all 44 major muscles in the model must move like human muscles. Dr. Eckman has classified six basic emotions- happiness, surprise, anger, disgust, fear, and sadness - and then assosiated each with characteristic movements of the facial muscles. \"The facial expressions for each emotion are universal\" observes Dr. Haung.\n\nA classroom computer could recognize when a student is becoming confused or bored, Dr. Haung predicts. Then it could modify the lesson, like an effective human instructor. Most human communication is nonverval, including emotional communication, notes Dr. Haung. According to the Facila Feedback Theory of Emotion, muving your facial muscles not only expresses emotions, but also help to produce them. whoever thought that making faces could reveal so much about the scien of emotions.\n\nCONCLUSION\n\nIn conclusion i tink that this program fantastic because can know how we feal, it identify human emotions just looking our face, it is a nice system because i can know if one person that a love is sad, happy, angry, etc. i thik that this system is goin to be important in the future.\"\"\"\nprediction = predict_essay(example_essay_ai)\nprint(f\"The essay is predicted to be: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:50:17.637309Z","iopub.execute_input":"2024-05-22T13:50:17.637724Z","iopub.status.idle":"2024-05-22T13:50:17.649945Z","shell.execute_reply.started":"2024-05-22T13:50:17.637684Z","shell.execute_reply":"2024-05-22T13:50:17.648577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_essay_human = \"\"\"Dear Senator [Last Name],\n\nI am writing to discuss the current method the United States uses to elect the president: the Electoral College. Some citizens, including me, have expressed concerns about this system, and there seems to be an increasing sentiment in favor of shifting to a popular vote method. Through this letter, I wish to voice my reasons for supporting this shift, while also giving due weight to counterarguments.\n\nThe Electoral College, as explained by the Office of the Federal Register, is an indirect voting system where citizens vote for electors, who then elect the President (Text 1, Paragraph 2). However, as Bradford Plumer argues in his article, \"The Indefensible Electoral College\", this method can lead to results that do not align with the popular vote, causing a president to be elected with minority support (Text 9). Plumer moreover points out the \"disaster factor,\" wherein potential issues with electors, or an electoral tie, could significantly disrupt the election process (Text 11, Text 12).\n\nOn the opposing side, Judge Richard A. Posner provides several practical reasons in defense of the Electoral College. He mentions its function in ensuring a candidate's appeal beyond specific regions, focusing attention on the most thoughtful and deciding voters, balancing the political weight of large states, and avoiding possibilities of run-off elections (Text 15-22). While these are valid points, I contend they do not outweigh the fundamental principle of democracy that each citizen's vote should hold equal influence.\n\nIt is unfair, as Plumer suggests, that due to the winner-take-all system, residents in certain states are seldom prioritized by the candidates, who instead focus on ' swing' states (Text 13). This reality disengages many potential voters who feel their votes will not count. The Electoral College does not incentivize the candidates to broaden their appeal to voters in “safe” states.\n\nFurthermore, the current system's opacity often confuses voters, leading to accidental voting for the wrong candidate (Text 10). In contrast, a direct popular vote method is comparatively more transparent and empowering for the citizens, promoting a more inclusive democratic process.\n\nI acknowledge that removing the Electoral College, fixed in the Constitution, entails a significant and complex alteration in our nation's structure, and therefore should not be hastily enacted without thorough examination and debate. However, I implore you to consider promoting discussions on this potential transition towards direct popular vote to ensure that every citizen's voice is equally valued in our democracy.\n\nI look forward to your thoughtful consideration on this matter.\n\nBest Regards, \n\n[Your Name]\"\"\"\nprediction = predict_essay(example_essay_human)\nprint(f\"The essay is predicted to be: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T13:51:25.037869Z","iopub.execute_input":"2024-05-22T13:51:25.038358Z","iopub.status.idle":"2024-05-22T13:51:25.051200Z","shell.execute_reply.started":"2024-05-22T13:51:25.038325Z","shell.execute_reply":"2024-05-22T13:51:25.049931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}